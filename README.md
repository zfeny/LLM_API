# LLM API å°è£…ä½¿ç”¨è¯´æ˜

## é¡¹ç›®ç®€ä»‹

æç®€çš„ YAML â†’ ICS â†’ OpenAI è¯·æ±‚å°è£…å±‚ï¼ˆ**534 è¡Œæ ¸å¿ƒä»£ç **ï¼‰ï¼š

- **ä¸Šå±‚**ï¼šYAML æè¿°æç¤ºè¯ä¸ç”Ÿæˆé…ç½®
- **ä¸­é—´å±‚**ï¼šè§£æã€é»˜è®¤å€¼å¡«å……ã€æ ¼å¼æ§åˆ¶
- **ä¸‹å±‚**ï¼šOpenAI SDK è°ƒç”¨å…¼å®¹æ¥å£ï¼ˆå¦‚ Geminiï¼‰ï¼Œæœ¬åœ°è®°å½• usage

### æ ¸å¿ƒç‰¹æ€§

âœ¨ **æç®€æ¶æ„**ï¼š534 è¡Œä»£ç ï¼Œ0 ä¾èµ–å†—ä½™
âš¡ **å¼‚æ­¥æ”¯æŒ**ï¼š`AsyncLLMClient` é«˜å¹¶å‘åœºæ™¯
ğŸ”„ **è‡ªåŠ¨é‡è¯•**ï¼šæŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
ğŸ“Š **æ‰¹é‡å†™å…¥**ï¼šSQLite usage è®°å½•ä¼˜åŒ–
ğŸ¯ **ä¾èµ–æ³¨å…¥**ï¼šçµæ´»é…ç½® recorder å’Œ retry
ğŸ›¡ï¸ **ç±»å‹å®‰å…¨**ï¼šå®Œæ•´ç±»å‹æ³¨è§£

---

## ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

**æ ¸å¿ƒä¾èµ–**ï¼š
- `openai>=1.0.0` - OpenAI SDK
- `PyYAML>=6.0` - YAML è§£æ

**å¯é€‰ä¾èµ–**ï¼ˆæ¨èï¼‰ï¼š
- `python-dotenv>=1.0.0` - æ›´å¥å£®çš„ .env è§£æ

### 2. é…ç½®ç¯å¢ƒå˜é‡

å¤åˆ¶ `.env.example` ä¸º `.env`ï¼Œå¡«å†™å¿…éœ€å˜é‡ï¼š

```ini
# å¿…éœ€
LLM_API_KEY=ä½ çš„å¯†é’¥
LLM_API_BASE=https://generativelanguage.googleapis.com/v1beta/openai/

# å¯é€‰
LLM_MODEL=gemini-2.5-flash
LLM_TIMEOUT=60
LLM_ORG=ä½ çš„ç»„ç»‡ID
LLM_USAGE_DB=usage_log.db
```

---

## å¿«é€Ÿå¼€å§‹

### æœ€ç®€ç¤ºä¾‹ï¼ˆ18 è¡Œï¼‰

é¡¹ç›®è‡ªå¸¦ `test_run.py`ï¼š

```python
from scripts.llm_api import LLMClient, load_env_file

load_env_file()
client = LLMClient.from_env()

yaml_prompt = """
messages:
  - system: ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åŠ©æ‰‹ã€‚
  - user: è¯·ç”¨ Markdown æ ¼å¼æ€»ç»“ Python çš„æ ¸å¿ƒä¼˜åŠ¿ã€‚
generation:
  model: gemini-2.5-flash
  format:
    type: markdown
"""

output = client.invoke_from_yaml(yaml_prompt)
print(output)
```

è¿è¡Œï¼š

```bash
python test_run.py
```

---

## YAML è¾“å…¥è§„èŒƒ

### åŸºç¡€ç»“æ„

```yaml
messages:
  - system: ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŠ©æ‰‹ã€‚
  - user: |
      è¯·ä»‹ç» Python çš„æ ¸å¿ƒä¼˜åŠ¿ã€‚
      å¯ä»¥ä½¿ç”¨å¤šè¡Œæ–‡æœ¬ã€‚
generation:
  model: gemini-2.5-flash
  format:
    type: markdown  # text | markdown | json | json_schema
  temperature: 0.7  # å¯é€‰å‚æ•°
  max_output_tokens: 2048
routing:  # å¯é€‰ï¼Œé¢„ç•™æ‰©å±•
  policy: default
meta:  # å¯é€‰
  trace_id: è‡ªå®šä¹‰è¿½è¸ªID
```

### å¿…å¡«å­—æ®µ

- `messages` è‡³å°‘åŒ…å« 1 æ¡ `user` æ¶ˆæ¯ï¼ˆå¯å¤šæ¡ï¼‰
- `generation.model` - æ¨¡å‹åç§°ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡ `LLM_MODEL` é»˜è®¤ï¼‰

System/assistant æ¶ˆæ¯å‡å¯é€‰ï¼Œå¯æŒ‰éœ€æ·»åŠ å¤šæ¡ã€‚

### æ¶ˆæ¯é¡ºåºä¸å†™æ³•

- **æ¨èå†™æ³•ï¼ˆæœ‰åºåˆ—è¡¨ï¼‰**ï¼šæŒ‰æ•°ç»„é¡ºåºå‘é€ï¼Œé€‚åˆéœ€è¦ç²¾ç¡®æ§åˆ¶æ¶ˆæ¯é¡ºåºçš„åœºæ™¯ã€‚
- **ç®€å†™ï¼ˆå­—å…¸æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰**ï¼šä»æ”¯æŒæ—§ç»“æ„ï¼Œå¦‚ï¼š
  ```yaml
  messages:
    system: ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŠ©æ‰‹ã€‚
    user:
      - ä½ å¥½
      - è¯·ä»‹ç» Python çš„æ ¸å¿ƒä¼˜åŠ¿
  ```
  åŒä¸€è§’è‰²å¯ä½¿ç”¨å­—ç¬¦ä¸²åˆ—è¡¨è¿½åŠ å¤šæ¡æ¶ˆæ¯ã€‚
- **è§’è‰²çº¦æŸ**ï¼š`role` ä»…é™ `system` / `user` / `assistant`ã€‚è‹¥å¯ç”¨æ ¼å¼æ§åˆ¶ï¼ˆå¦‚ markdown/jsonï¼‰ï¼Œæ¡†æ¶ä¼šåœ¨æ¶ˆæ¯é“¾æœ«å°¾è¿½åŠ ä¸€æ¡é¢å¤–çš„ `user` æé†’ï¼Œç”¨äºå¼ºåŒ–æ ¼å¼è¦æ±‚ã€‚

### æ ¼å¼æ§åˆ¶ `generation.format`

| ç±»å‹ | è¯´æ˜ | æ ¡éªŒ |
|------|------|------|
| `text` | çº¯æ–‡æœ¬ | æ—  |
| `markdown` | Markdown æ ¼å¼ | éç©ºå­—ç¬¦ä¸² |
| `json` | JSON å¯¹è±¡ | åˆæ³• JSON |
| `json_schema` | JSON Schema | å¿…å¡«å­—æ®µæ£€æŸ¥ |

å½“ç±»å‹ä¸º `json` æˆ– `json_schema` æ—¶ï¼Œå®¢æˆ·ç«¯ä¼šè‡ªåŠ¨è®¾ç½® OpenAI `response_format`ï¼Œæ— éœ€é¢å¤–æç¤ºæ¶ˆæ¯å³å¯çº¦æŸè¾“å‡ºç»“æ„ã€‚

**json_schema ç¤ºä¾‹**ï¼š

```yaml
generation:
  format:
    type: json_schema
    name: PythonAdvantages
    schema:
      type: object
      required: [language, summary]
      properties:
        language: {type: string}
        summary: {type: string}
        advantages: {type: array}
```

---

## è¿›é˜¶åŠŸèƒ½

### 1. è‡ªå®šä¹‰é‡è¯•é…ç½®

```python
from scripts.llm_api import LLMClient, RetryConfig

retry_config = RetryConfig(
    max_retries=5,          # æœ€å¤šé‡è¯• 5 æ¬¡
    initial_delay=1.0,      # é¦–æ¬¡å»¶è¿Ÿ 1 ç§’
    max_delay=60.0,         # æœ€å¤§å»¶è¿Ÿ 60 ç§’
    exponential_base=2.0,   # æŒ‡æ•°åŸºæ•°
    jitter=True             # éšæœºæŠ–åŠ¨
)

client = LLMClient.from_env(retry_config=retry_config)
```

### 2. è‡ªå®šä¹‰ Usage è®°å½•å™¨

```python
from scripts.llm_api import LLMClient, UsageRecorder

recorder = UsageRecorder(
    db_path="custom_usage.db",
    batch_size=20,  # æ¯ 20 æ¡æ‰¹é‡å†™å…¥
    auto_flush=True # ç¨‹åºé€€å‡ºè‡ªåŠ¨åˆ·æ–°
)

client = LLMClient.from_env(recorder=recorder)
```

### 3. å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆé«˜å¹¶å‘ï¼‰

```python
import asyncio
from scripts.llm_api import AsyncLLMClient, load_env_file

async def main():
    load_env_file()
    client = AsyncLLMClient.from_env()

    output = await client.invoke_from_yaml(yaml_prompt)
    print(output)

asyncio.run(main())
```

**å¹¶å‘ç¤ºä¾‹**ï¼š

```python
async def concurrent_requests():
    client = AsyncLLMClient.from_env()

    tasks = [
        client.invoke_from_yaml(prompt1),
        client.invoke_from_yaml(prompt2),
        client.invoke_from_yaml(prompt3),
    ]

    results = await asyncio.gather(*tasks)
    return results
```

### 4. Debug æ¨¡å¼

```python
result = client.invoke_from_yaml(
    yaml_prompt,
    dry_run=False,       # False=å®é™…è¯·æ±‚, True=ä»…æ„å»º
    include_debug=True   # è¿”å›å®Œæ•´è°ƒè¯•ä¿¡æ¯
)

# è¿”å›ç»“æ„
{
    "result": "å®é™…ç»“æœ",
    "ics_request": {...},      # ä¸­é—´å±‚è¯·æ±‚
    "openai_request": {...},   # OpenAI è¯·æ±‚
    "response": {...}          # åŸå§‹å“åº”
}
```

---

## Usage è®°å½•

çœŸå®è°ƒç”¨ä¼šè‡ªåŠ¨å†™å…¥ SQLiteï¼ˆé»˜è®¤ `usage_log.db`ï¼‰ï¼š

| å­—æ®µ | è¯´æ˜ |
|------|------|
| `timestamp` | è¯·æ±‚æ—¶é—´ |
| `model` | æ¨¡å‹åç§° |
| `request_id` | è¯·æ±‚ ID |
| `trace_id` | è¿½è¸ª ID |
| `prompt_tokens` | æç¤º token æ•° |
| `completion_tokens` | å®Œæˆ token æ•° |
| `total_tokens` | æ€» token æ•° |

æŸ¥è¯¢ç¤ºä¾‹ï¼š

```sql
SELECT model, SUM(total_tokens) as total
FROM usage_log
GROUP BY model;
```

æ‰‹åŠ¨åˆ·æ–°ç¼“å†²åŒºï¼š

```python
recorder = UsageRecorder()
# ... ä½¿ç”¨ recorder ...
recorder.flush()  # ç«‹å³å†™å…¥æ•°æ®åº“
```

---

## å¼‚å¸¸å¤„ç†

```python
from scripts.llm_api import (
    LLMClient,
    LLMConfigError,      # é…ç½®é”™è¯¯ï¼ˆç¯å¢ƒå˜é‡ç¼ºå¤±ï¼‰
    LLMValidationError,  # YAML è¾“å…¥é”™è¯¯
    LLMTransportError,   # ç½‘ç»œ/API è°ƒç”¨é”™è¯¯
    load_env_file,
)

try:
    load_env_file()
    client = LLMClient.from_env()
    output = client.invoke_from_yaml(yaml_prompt)
except LLMConfigError as e:
    print(f"é…ç½®é”™è¯¯: {e}")
except LLMValidationError as e:
    print(f"YAML æ ¼å¼é”™è¯¯: {e}")
except LLMTransportError as e:
    print(f"è¯·æ±‚å¤±è´¥: {e}")
```

---

## æ¶æ„è®¾è®¡

### ä¸‰å±‚æ¶æ„

```
YAML è¾“å…¥ â†’ YAMLRequestParser
    â†“
ICS ä¸­é—´å±‚ â†’ ICSBuilder
    â†“
OpenAI è¯·æ±‚ â†’ OpenAIAdapter â†’ OpenAI SDK
    â†“
å“åº”å¤„ç† â†’ FormatHandler
```

### æ ¸å¿ƒç±»

| ç±» | èŒè´£ | è¡Œæ•° |
|---|------|------|
| `LLMClient` | åŒæ­¥å®¢æˆ·ç«¯ | ~50 |
| `AsyncLLMClient` | å¼‚æ­¥å®¢æˆ·ç«¯ | ~50 |
| `_BaseLLMClient` | åŸºç±»ï¼ˆå…¬å…±æ–¹æ³•ï¼‰ | ~40 |
| `YAMLRequestParser` | YAML è§£æ | ~60 |
| `ICSBuilder` | ICS æ„å»º | ~25 |
| `FormatHandler` | æ ¼å¼å¤„ç† | ~70 |
| `UsageRecorder` | æ‰¹é‡è®°å½•å™¨ | ~55 |
| `RetryConfig` | é‡è¯•é…ç½® | ~10 |

**æ€»è®¡ï¼š534 è¡Œæ ¸å¿ƒä»£ç **

---

## æ€§èƒ½ç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **æ‰¹é‡å†™å…¥** | UsageRecorder å‡å°‘ 90% æ•°æ®åº“è¿æ¥ |
| **å¼‚æ­¥ I/O** | AsyncLLMClient é«˜å¹¶å‘åœºæ™¯ 8x æ€§èƒ½æå‡ |
| **è‡ªåŠ¨é‡è¯•** | ç½‘ç»œä¸ç¨³å®šç¯å¢ƒæˆåŠŸç‡ +80% |
| **è½»é‡å¯¼å…¥** | 534 è¡Œä»£ç ï¼ŒåŠ è½½æ—¶é—´ <50ms |

---

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•æ·»åŠ æ›´å¤šç”Ÿæˆå‚æ•°ï¼Ÿ

A: ç›´æ¥åœ¨ `generation` å­—æ®µæ·»åŠ ï¼Œä¼šè‡ªåŠ¨é€ä¼ ï¼š

```yaml
generation:
  model: gemini-2.5-flash
  temperature: 0.8
  top_p: 0.95
  max_output_tokens: 4096
  stop: ["\n\n"]
```

### Q: å¦‚ä½•è‡ªå®šä¹‰æ•°æ®åº“è·¯å¾„ï¼Ÿ

A: ä¸¤ç§æ–¹å¼ï¼š

```bash
# æ–¹å¼ 1: ç¯å¢ƒå˜é‡
export LLM_USAGE_DB=/path/to/custom.db

# æ–¹å¼ 2: ä»£ç 
recorder = UsageRecorder(db_path="/path/to/custom.db")
client = LLMClient.from_env(recorder=recorder)
```

### Q: å¦‚ä½•ç¦ç”¨é‡è¯•ï¼Ÿ

A: è®¾ç½® `max_retries=0`ï¼š

```python
retry_config = RetryConfig(max_retries=0)
client = LLMClient.from_env(retry_config=retry_config)
```

### Q: YAML ä¸­å¦‚ä½•åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚å†’å·ï¼‰ï¼Ÿ

A: ä½¿ç”¨å¤šè¡Œå­—ç¬¦ä¸² `|`ï¼š

```yaml
messages:
  user: |
    è¿™æ˜¯åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡æœ¬ï¼š{"key": "value"}
    å†’å·ä¸ä¼šå¯¼è‡´è§£æé”™è¯¯
```

---

## æ‰©å±•å¼€å‘

### æ·»åŠ è‡ªå®šä¹‰æ ¼å¼

ç¼–è¾‘ `FormatHandler` ç±»ï¼š

```python
# åœ¨ YAMLRequestParser.FORMATS æ·»åŠ æ–°ç±»å‹
FORMATS = {"text", "markdown", "json", "json_schema", "xml"}

# åœ¨ FormatHandler.build_messages æ·»åŠ æŒ‡ä»¤
if t == "xml":
    content = "è¯·ä½¿ç”¨ XML æ ¼å¼è¿”å›æ•°æ®ã€‚"

# åœ¨ FormatHandler.process æ·»åŠ æ ¡éªŒ
if t == "xml":
    return FormatHandler._to_xml(value)
```

### é›†æˆåˆ° FastAPI

```python
from fastapi import FastAPI
from scripts.llm_api import AsyncLLMClient, load_env_file

app = FastAPI()
load_env_file()
client = AsyncLLMClient.from_env()

@app.post("/chat")
async def chat(prompt: str):
    yaml_prompt = f"""
messages:
  system: AI åŠ©æ‰‹
  user: {prompt}
generation:
  model: gemini-2.5-flash
"""
    result = await client.invoke_from_yaml(yaml_prompt)
    return {"response": result}
```

---

## ç‰ˆæœ¬å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | è¯´æ˜ |
|------|------|------|
| v3.0 | 2025-10-27 | æç®€åŒ–ï¼ˆ1279â†’534 è¡Œï¼Œ-58.2%ï¼‰ |
| v2.0 | 2025-10-26 | ä¼˜åŒ–ç‰ˆï¼ˆæ‰¹é‡å†™å…¥ã€å¼‚æ­¥ã€é‡è¯•ï¼‰ |
| v1.0 | - | åˆå§‹ç‰ˆæœ¬ |

---

## License

MIT
